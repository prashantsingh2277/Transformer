# Transformer
Built a custom Transformer model from scratch for sequence-to-sequence tasks using PyTorch, incorporating multi-head attention, positional encoding, and feed-forward layers. Optimized training with gradient clipping, learning rate scheduling, and mixed-precision techniques for improved performance and memory efficiency.
